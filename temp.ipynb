{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Transformers, can you rate the complexity of reading passages?](https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403?sk=0fc1d1199174a065636c186e90342c90)\n### Fine-tuning RoBERTa with PyTorch to predict reading ease of text excerpts\n\n## [Click here to learn and read about these advanced techniques and see how they can help improve results](https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e?sk=ef155ae94d003aadb288f3f2c5b8e4ae) \n","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn import metrics\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch, os, gc, random\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim.swa_utils import AveragedModel, SWALR\n\nimport transformers\nfrom transformers import RobertaTokenizer, RobertaModel\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-14T08:41:25.033209Z","iopub.execute_input":"2021-09-14T08:41:25.033563Z","iopub.status.idle":"2021-09-14T08:41:25.039897Z","shell.execute_reply.started":"2021-09-14T08:41:25.033535Z","shell.execute_reply":"2021-09-14T08:41:25.038712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import __version__\nprint(\"Transformers version:\", __version__)\nprint(\"PyTorch version:\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.075145Z","iopub.execute_input":"2021-09-14T08:41:25.07541Z","iopub.status.idle":"2021-09-14T08:41:25.083374Z","shell.execute_reply.started":"2021-09-14T08:41:25.075387Z","shell.execute_reply":"2021-09-14T08:41:25.082438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')    ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.122404Z","iopub.execute_input":"2021-09-14T08:41:25.122651Z","iopub.status.idle":"2021-09-14T08:41:25.132557Z","shell.execute_reply.started":"2021-09-14T08:41:25.122628Z","shell.execute_reply":"2021-09-14T08:41:25.131546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://pytorch.org/docs/stable/notes/randomness.html\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.142665Z","iopub.execute_input":"2021-09-14T08:41:25.142936Z","iopub.status.idle":"2021-09-14T08:41:25.147248Z","shell.execute_reply.started":"2021-09-14T08:41:25.142913Z","shell.execute_reply":"2021-09-14T08:41:25.146137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Seed","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed):    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)    ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.166155Z","iopub.execute_input":"2021-09-14T08:41:25.166395Z","iopub.status.idle":"2021-09-14T08:41:25.172095Z","shell.execute_reply.started":"2021-09-14T08:41:25.166373Z","shell.execute_reply":"2021-09-14T08:41:25.17119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# You may need to change the path to where the file resides.\n\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nprint(\"Training dataset shape : \" + str(df.shape))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.18798Z","iopub.execute_input":"2021-09-14T08:41:25.188251Z","iopub.status.idle":"2021-09-14T08:41:25.228597Z","shell.execute_reply.started":"2021-09-14T08:41:25.188228Z","shell.execute_reply":"2021-09-14T08:41:25.227673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview of the dataset.\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.229815Z","iopub.execute_input":"2021-09-14T08:41:25.230166Z","iopub.status.idle":"2021-09-14T08:41:25.244914Z","shell.execute_reply.started":"2021-09-14T08:41:25.230131Z","shell.execute_reply":"2021-09-14T08:41:25.244106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph below we can see that there's a record with 0 `target`, and 0 `standard error` (measure of spread of scores among multiple raters for each `excerpt`). The `excerpt` from this record is basically the baseline for all other comparisons.\n\nWe will remove this record from the dataset.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x = df[\"target\"], y = df[\"standard_error\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.275569Z","iopub.execute_input":"2021-09-14T08:41:25.27581Z","iopub.status.idle":"2021-09-14T08:41:25.40265Z","shell.execute_reply.started":"2021-09-14T08:41:25.275787Z","shell.execute_reply":"2021-09-14T08:41:25.401629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove record with 0 target and 0 standard_error.\n\ndf.drop(df[(df.target == 0) & (df.standard_error == 0)].index, inplace=True)\ndf.reset_index(drop=True, inplace=True)\n\nprint(\" Training dataset shape : \" + str(df.shape))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.404143Z","iopub.execute_input":"2021-09-14T08:41:25.404497Z","iopub.status.idle":"2021-09-14T08:41:25.413163Z","shell.execute_reply.started":"2021-09-14T08:41:25.404461Z","shell.execute_reply":"2021-09-14T08:41:25.412152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The target column values ranges from -3.676268 to 1.711390\n# with mean of -0.959657.\n\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.415418Z","iopub.execute_input":"2021-09-14T08:41:25.415895Z","iopub.status.idle":"2021-09-14T08:41:25.436153Z","shell.execute_reply.started":"2021-09-14T08:41:25.415843Z","shell.execute_reply":"2021-09-14T08:41:25.435063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data into Training Set & Validation Set","metadata":{}},{"cell_type":"code","source":"set_random_seed(3377)\ndf[\"skfold\"] = -99\n\ndf = df.sample(frac=1).reset_index(drop=True)\nbins = int(np.floor(1 + np.log2(len(df))))\ndf.loc[:, \"bins\"] = pd.cut(df[\"target\"], bins=bins, labels=False)\nskf = StratifiedKFold(n_splits = 5)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=df[\"bins\"].values)):\n    df.loc[val_idx, \"skfold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.43791Z","iopub.execute_input":"2021-09-14T08:41:25.438339Z","iopub.status.idle":"2021-09-14T08:41:25.455392Z","shell.execute_reply.started":"2021-09-14T08:41:25.438302Z","shell.execute_reply":"2021-09-14T08:41:25.454512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of records under each fold.\ndf[\"skfold\"].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.456572Z","iopub.execute_input":"2021-09-14T08:41:25.456941Z","iopub.status.idle":"2021-09-14T08:41:25.466964Z","shell.execute_reply.started":"2021-09-14T08:41:25.456907Z","shell.execute_reply":"2021-09-14T08:41:25.46594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For each fold, the mean target is almost consistent.\n# They are very close to the mean target -0.959657 for the entire dataset.\n\ndf.groupby(\"skfold\")[\"target\"].mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.468488Z","iopub.execute_input":"2021-09-14T08:41:25.469129Z","iopub.status.idle":"2021-09-14T08:41:25.478366Z","shell.execute_reply.started":"2021-09-14T08:41:25.469003Z","shell.execute_reply":"2021-09-14T08:41:25.477178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(8,5))\nfor fold in range(max(df[\"skfold\"])+1):\n    sns.distplot(df[df[\"skfold\"]==fold][\"target\"], \n                 hist=False,\n                 kde = True,\n                 label = f\"Fold {fold}\"\n                )\n    \nplt.title(f\"Target Distribution for Each Fold\")\nplt.legend(loc=\"best\") \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.479931Z","iopub.execute_input":"2021-09-14T08:41:25.480371Z","iopub.status.idle":"2021-09-14T08:41:25.720038Z","shell.execute_reply.started":"2021-09-14T08:41:25.480338Z","shell.execute_reply":"2021-09-14T08:41:25.719114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class MyDataset(torch.utils.data.Dataset):\n   \n    def __init__(self, texts, targets, tokenizer, seq_len=250):        \n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n    \n    def __len__(self):\n        \"\"\"Returns the length of dataset.\"\"\"\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        For a given item index, return a dictionary of encoded information        \n        \"\"\"        \n        text = str(self.texts[idx]) \n        \n        tokenized = self.tokenizer(\n            text,            \n            max_length = self.seq_len,                                \n            padding = \"max_length\",     # Pad to the specified max_length. \n            truncation = True,          # Truncate to the specified max_length. \n            add_special_tokens = True,  # Whether to insert [CLS], [SEP], <s>, etc.   \n            return_attention_mask = True            \n        )     \n        \n        return {\"ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long),\n                \"masks\": torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long),\n                \"target\": torch.tensor(self.targets[idx], dtype=torch.float)\n               }","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.722802Z","iopub.execute_input":"2021-09-14T08:41:25.723158Z","iopub.status.idle":"2021-09-14T08:41:25.730806Z","shell.execute_reply.started":"2021-09-14T08:41:25.723122Z","shell.execute_reply":"2021-09-14T08:41:25.730009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class MyModel(nn.Module):\n            \n    def __init__(self, reinit_n_layers=0):        \n        super().__init__() \n        self.roberta_model = RobertaModel.from_pretrained('roberta-base')       \n        self.regressor = nn.Linear(768, 1)  \n        self.reinit_n_layers = reinit_n_layers\n        if reinit_n_layers > 0: self._do_reinit()            \n            \n    def _debug_reinit(self, text):\n        print(f\"\\n{text}\\nPooler:\\n\", self.roberta_model.pooler.dense.weight.data)        \n        for i, layer in enumerate(self.roberta_model.encoder.layer[-self.reinit_n_layers:]):\n            for module in layer.modules():\n                if isinstance(module, nn.Linear):\n                    print(f\"\\n{i} nn.Linear:\\n\", module.weight.data) \n                elif isinstance(module, nn.LayerNorm):\n                    print(f\"\\n{i} nn.LayerNorm:\\n\", module.weight.data) \n        \n    def _do_reinit(self):\n        # Re-init pooler.\n        self.roberta_model.pooler.dense.weight.data.normal_(mean=0.0, std=self.roberta_model.config.initializer_range)\n        self.roberta_model.pooler.dense.bias.data.zero_()\n        for param in self.roberta_model.pooler.parameters():\n            param.requires_grad = True\n        \n        # Re-init last n layers.\n        for n in range(self.reinit_n_layers):\n            self.roberta_model.encoder.layer[-(n+1)].apply(self._init_weight_and_bias)\n            \n    def _init_weight_and_bias(self, module):                        \n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.roberta_model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n#         elif isinstance(module, nn.Embedding):\n#             module.weight.data.normal_(mean=0.0, std=self.roberta_model.config.initializer_range)\n#             if module.padding_idx is not None:\n#                 module.weight.data[module.padding_idx].zero_()\n \n    def forward(self, input_ids, attention_mask):        \n        raw_output = self.roberta_model(input_ids, attention_mask, return_dict=True)        \n        pooler = raw_output[\"pooler_output\"]    # Shape is [batch_size, 768]\n        output = self.regressor(pooler)         # Shape is [batch_size, 1]\n        return output ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.732722Z","iopub.execute_input":"2021-09-14T08:41:25.733354Z","iopub.status.idle":"2021-09-14T08:41:25.748246Z","shell.execute_reply.started":"2021-09-14T08:41:25.733316Z","shell.execute_reply":"2021-09-14T08:41:25.747442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    \n    def __init__(self, input_dim=768, hidden_dim=512):\n        super().__init__()\n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, 1)\n\n    def forward(self, last_hidden_state):\n        \"\"\"\n        Note:\n        \"last_hidden_state\" shape is [batch_size, seq_len, 768].\n        The \"weights\" produced from softmax will add up to 1 across all tokens in each record.\n        \"\"\"        \n        linear1_output = self.linear1(last_hidden_state)  # Shape is [batch_size, seq_len, 512]  \n        activation = torch.tanh(linear1_output)           # Shape is [batch_size, seq_len, 512]        \n        score = self.linear2(activation)                  # Shape is [batch_size, seq_len, 1]        \n        weights = torch.softmax(score, dim=1)             # Shape is [batch_size, seq_len, 1]              \n        result = torch.sum(weights * last_hidden_state, dim=1) # Shape is [batch_size, 768]          \n        return result","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.751039Z","iopub.execute_input":"2021-09-14T08:41:25.751325Z","iopub.status.idle":"2021-09-14T08:41:25.762919Z","shell.execute_reply.started":"2021-09-14T08:41:25.7513Z","shell.execute_reply":"2021-09-14T08:41:25.762048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyModel_AttnHead(nn.Module):\n            \n    def __init__(self):        \n        super().__init__() \n        self.roberta_model = RobertaModel.from_pretrained('roberta-base')\n        self.attn_head = AttentionHead(768, 512)       \n        self.regressor = nn.Linear(768, 1)   \n            \n    def forward(self, input_ids, attention_mask):       \n        raw_output = self.roberta_model(input_ids, attention_mask, return_dict=True)        \n        last_hidden_state = raw_output[\"last_hidden_state\"] # Shape is [batch_size, seq_len, 768]\n        attn = self.attn_head(last_hidden_state)            # Shape is [batch_size, 768]\n        output = self.regressor(attn)                       # Shape is [batch_size, 1]       \n        return output ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.764225Z","iopub.execute_input":"2021-09-14T08:41:25.764651Z","iopub.status.idle":"2021-09-14T08:41:25.776329Z","shell.execute_reply.started":"2021-09-14T08:41:25.764614Z","shell.execute_reply":"2021-09-14T08:41:25.775471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyModel_ConcatLast4Layers(nn.Module):\n            \n    def __init__(self):        \n        super().__init__() \n        self.roberta_model = RobertaModel.from_pretrained('roberta-base')\n        self.regressor = nn.Linear(768*4, 1)     \n    \n    def forward(self, input_ids, attention_mask):       \n        raw_output = self.roberta_model(input_ids, attention_mask, \n                                        return_dict=True, output_hidden_states=True)        \n        hidden_states = raw_output[\"hidden_states\"] \n        hidden_states = torch.stack(hidden_states) # Shape is [13, batch_size, seq_len, 768]\n        concat = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1) \n                                             # Shape is [batch_size, seq_len, 768*4]\n        first_token = concat[:, 0, :]        # Take only 1st token, result in shape [batch_size, 768*4]\n        output = self.regressor(first_token) # Shape is [batch_size, 1]    \n        return output ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.779235Z","iopub.execute_input":"2021-09-14T08:41:25.779567Z","iopub.status.idle":"2021-09-14T08:41:25.788571Z","shell.execute_reply.started":"2021-09-14T08:41:25.779533Z","shell.execute_reply":"2021-09-14T08:41:25.787792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Util","metadata":{}},{"cell_type":"code","source":"def collect_lr_by_layers(optimizer, grouped_LLRD=True):    \n    lr = []\n    if grouped_LLRD:\n        lr.append(optimizer.param_groups[0][\"lr\"])   # embeddings\n        lr.append(optimizer.param_groups[5][\"lr\"])   # layer0\n        lr.append(optimizer.param_groups[21][\"lr\"])  # layer1\n        lr.append(optimizer.param_groups[37][\"lr\"])  # layer2\n        lr.append(optimizer.param_groups[53][\"lr\"])  # layer3\n        lr.append(optimizer.param_groups[69][\"lr\"])  # layer4\n        lr.append(optimizer.param_groups[85][\"lr\"])  # layer5\n        lr.append(optimizer.param_groups[101][\"lr\"]) # layer6\n        lr.append(optimizer.param_groups[117][\"lr\"]) # layer7\n        lr.append(optimizer.param_groups[133][\"lr\"]) # layer8\n        lr.append(optimizer.param_groups[149][\"lr\"]) # layer9\n        lr.append(optimizer.param_groups[165][\"lr\"]) # layer10\n        lr.append(optimizer.param_groups[181][\"lr\"]) # layer11\n        lr.append(optimizer.param_groups[197][\"lr\"]) # pooler\n        lr.append(optimizer.param_groups[199][\"lr\"]) # regressor \n    else:\n        lr.append(optimizer.param_groups[26][\"lr\"]) # embeddings\n        lr.append(optimizer.param_groups[24][\"lr\"]) # layer0\n        lr.append(optimizer.param_groups[22][\"lr\"]) # layer1\n        lr.append(optimizer.param_groups[20][\"lr\"]) # layer2\n        lr.append(optimizer.param_groups[18][\"lr\"]) # layer3\n        lr.append(optimizer.param_groups[16][\"lr\"]) # layer4\n        lr.append(optimizer.param_groups[14][\"lr\"]) # layer5\n        lr.append(optimizer.param_groups[12][\"lr\"]) # layer6\n        lr.append(optimizer.param_groups[10][\"lr\"]) # layer7\n        lr.append(optimizer.param_groups[8][\"lr\"])  # layer8\n        lr.append(optimizer.param_groups[6][\"lr\"])  # layer9\n        lr.append(optimizer.param_groups[4][\"lr\"])  # layer10\n        lr.append(optimizer.param_groups[2][\"lr\"])  # layer11\n        lr.append(optimizer.param_groups[0][\"lr\"])  # pooler\n        lr.append(optimizer.param_groups[0][\"lr\"])  # regressor \n    return lr    ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.790426Z","iopub.execute_input":"2021-09-14T08:41:25.790695Z","iopub.status.idle":"2021-09-14T08:41:25.804374Z","shell.execute_reply.started":"2021-09-14T08:41:25.790664Z","shell.execute_reply":"2021-09-14T08:41:25.803565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roberta_base_AdamW_LLRD(model, debug=False):\n    \n    opt_parameters = [] # To be passed to the optimizer (only parameters of the layers you want to update).\n    named_parameters = list(model.named_parameters()) \n    debug_param_groups = []\n    \n    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n    # for bias and LayerNorm.weight layers.\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    init_lr = 3.5e-6 \n    head_lr = 3.6e-6\n    lr = init_lr\n    \n    # === Pooler and regressor ======================================================  \n    \n    params_0 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n) \n                and any(nd in n for nd in no_decay)]\n    params_1 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n                and not any(nd in n for nd in no_decay)]\n    \n    head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}    \n    opt_parameters.append(head_params)\n    debug_param_groups.append(f\"head_params\")\n    \n    head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": 0.01}    \n    opt_parameters.append(head_params)\n    debug_param_groups.append(f\"head_params\")\n            \n    # === 12 Hidden layers ==========================================================\n    \n    for layer in range(11,-1,-1):\n        \n        params_0 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n                    and any(nd in n for nd in no_decay)]\n        params_1 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n                    and not any(nd in n for nd in no_decay)]\n        \n        layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n        opt_parameters.append(layer_params)   \n        debug_param_groups.append(f\"layer.{layer}\")\n                    \n        layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n        opt_parameters.append(layer_params)\n        debug_param_groups.append(f\"layer.{layer}\")       \n        \n        lr *= 0.9 \n    \n    # === Embeddings layer ==========================================================\n    \n    params_0 = [p for n,p in named_parameters if \"embeddings\" in n \n                and any(nd in n for nd in no_decay)]\n    params_1 = [p for n,p in named_parameters if \"embeddings\" in n\n                and not any(nd in n for nd in no_decay)]\n    \n    embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n    opt_parameters.append(embed_params)\n    debug_param_groups.append(f\"embed_params\")\n    \n    embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n    opt_parameters.append(embed_params)\n    debug_param_groups.append(f\"embed_params\")\n    \n    if debug: \n        for g in range(len(debug_param_groups)): print(g, debug_param_groups[g]) \n    \n    return transformers.AdamW(opt_parameters, lr=init_lr), debug_param_groups","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.805871Z","iopub.execute_input":"2021-09-14T08:41:25.806281Z","iopub.status.idle":"2021-09-14T08:41:25.822014Z","shell.execute_reply.started":"2021-09-14T08:41:25.806248Z","shell.execute_reply":"2021-09-14T08:41:25.820817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roberta_base_AdamW_grouped_LLRD(model, debug=False):\n        \n    opt_parameters = [] # To be passed to the optimizer (only parameters of the layers you want to update).\n    debug_param_groups = []\n    named_parameters = list(model.named_parameters()) \n    \n    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n    # for bias and LayerNorm.weight layers.\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    set_2 = [\"layer.4\", \"layer.5\", \"layer.6\", \"layer.7\"]\n    set_3 = [\"layer.8\", \"layer.9\", \"layer.10\", \"layer.11\"]\n    init_lr = 1e-6\n    \n    for i, (name, params) in enumerate(named_parameters):  \n        \n        weight_decay = 0.0 if any(p in name for p in no_decay) else 0.01\n \n        if name.startswith(\"roberta_model.embeddings\") or name.startswith(\"roberta_model.encoder\"):            \n            # For first set, set lr to 1e-6 (i.e. 0.000001)\n            lr = init_lr       \n            \n            # For set_2, increase lr to 0.00000175\n            lr = init_lr * 1.75 if any(p in name for p in set_2) else lr\n            \n            # For set_3, increase lr to 0.0000035 \n            lr = init_lr * 3.5 if any(p in name for p in set_3) else lr\n            \n            opt_parameters.append({\"params\": params,\n                                   \"weight_decay\": weight_decay,\n                                   \"lr\": lr})  \n            \n        # For regressor and pooler, set lr to 0.0000036 (slightly higher than the top layer).                \n        if name.startswith(\"regressor\") or name.startswith(\"roberta_model.pooler\"):               \n            lr = init_lr * 3.6 \n            \n            opt_parameters.append({\"params\": params,\n                                   \"weight_decay\": weight_decay,\n                                   \"lr\": lr})    \n            \n        debug_param_groups.append(f\"{i} {name}\")\n\n    if debug: \n        for g in range(len(debug_param_groups)): print(debug_param_groups[g]) \n\n    return transformers.AdamW(opt_parameters, lr=init_lr), debug_param_groups","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.823425Z","iopub.execute_input":"2021-09-14T08:41:25.823859Z","iopub.status.idle":"2021-09-14T08:41:25.835765Z","shell.execute_reply.started":"2021-09-14T08:41:25.823824Z","shell.execute_reply":"2021-09-14T08:41:25.834864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(predictions, targets):       \n    return torch.sqrt(nn.MSELoss()(predictions, targets))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.837066Z","iopub.execute_input":"2021-09-14T08:41:25.837506Z","iopub.status.idle":"2021-09-14T08:41:25.847448Z","shell.execute_reply.started":"2021-09-14T08:41:25.837471Z","shell.execute_reply":"2021-09-14T08:41:25.846632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler, \n             grouped_LLRD, swa_model, swa_scheduler, swa_step=False):\n        \n    model.train()                               # Put the model in training mode.               \n    \n    lr_list = []\n    lr_list2 = []\n    train_losses = [] \n    df_lr = pd.DataFrame()\n       \n    for batch in data_loader:                   # Loop over all batches.\n        \n        ids = batch[\"ids\"].to(device, dtype=torch.long)\n        masks = batch[\"masks\"].to(device, dtype=torch.long)\n        targets = batch[\"target\"].to(device, dtype=torch.float) \n        \n        optimizer.zero_grad()                   # To zero out the gradients.\n        \n        outputs = model(ids, masks).squeeze(-1) # Predictions from 1 batch of data.\n        \n        loss = loss_fn(outputs, targets)        # Get the training loss.\n        train_losses.append(loss.item())\n\n        loss.backward()                         # To backpropagate the error (gradients are computed).\n        optimizer.step()                        # To update parameters based on current gradients.\n        \n        lr_list.append(optimizer.param_groups[0][\"lr\"])\n        lr_list2.append(collect_lr_by_layers(optimizer, grouped_LLRD))\n        \n        if swa_step:            \n            swa_model.update_parameters(model)  # To update parameters of the averaged model.\n            swa_scheduler.step()                # Switch to SWALR.\n        else:        \n            scheduler.step()                    # To update learning rate.\n               \n    return train_losses, lr_list, lr_list2","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.848663Z","iopub.execute_input":"2021-09-14T08:41:25.849297Z","iopub.status.idle":"2021-09-14T08:41:25.85937Z","shell.execute_reply.started":"2021-09-14T08:41:25.849257Z","shell.execute_reply":"2021-09-14T08:41:25.858473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate_fn(data_loader, model, device):\n        \n    model.eval()                                    # Put model in evaluation mode.\n    \n    val_losses = []\n        \n    with torch.no_grad():                           # Disable gradient calculation.\n        \n        for batch in data_loader:                   # Loop over all batches.   \n            \n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            masks = batch[\"masks\"].to(device, dtype=torch.long)\n            targets = batch[\"target\"].to(device, dtype=torch.float)\n\n            outputs = model(ids, masks).squeeze(-1) # Predictions from 1 batch of data.\n            \n            loss = loss_fn(outputs, targets)        # Get the validation loss.\n            val_losses.append(loss.item())\n            \n    return val_losses ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.860576Z","iopub.execute_input":"2021-09-14T08:41:25.860995Z","iopub.status.idle":"2021-09-14T08:41:25.870069Z","shell.execute_reply.started":"2021-09-14T08:41:25.860937Z","shell.execute_reply":"2021-09-14T08:41:25.869196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_validate(best_rmse, early_stopping_counter, fold, epoch,\n                       train_data_loader, val_data_loader, model, optimizer, device, scheduler, \n                       EARLY_STOP_THRESHOLD, grouped_LLRD, \n                       swa_model, swa_scheduler, swa_step=False):\n        \n    lr_list = []\n    lr_list2 = []\n    train_losses = [] \n    val_losses = []\n    train_loss = 0\n      \n    for i, batch in enumerate(train_data_loader):  # Loop over all training batches.\n        \n        model.train()                           # Put the model in training mode.      \n        \n        ids = batch[\"ids\"].to(device, dtype=torch.long)\n        masks = batch[\"masks\"].to(device, dtype=torch.long)\n        targets = batch[\"target\"].to(device, dtype=torch.float) \n        \n        optimizer.zero_grad()                   # To zero out the gradients.\n        \n        outputs = model(ids, masks).squeeze(-1) # Predictions from 1 batch of data.\n        \n        loss = loss_fn(outputs, targets)        # Get the training loss.\n        train_loss += loss.item()               # Add up the training losses.\n \n        loss.backward()                         # To backpropagate the error (gradients are computed).\n        optimizer.step()                        # To update parameters based on current gradients.\n        \n        lr_list.append(optimizer.param_groups[0][\"lr\"])\n        lr_list2.append(collect_lr_by_layers(optimizer, grouped_LLRD))\n        \n        if swa_step:            \n            swa_model.update_parameters(model)  # To update parameters of the averaged model.\n            swa_scheduler.step()                # Switch to SWALR.\n        else:        \n            scheduler.step()                    # To update learning rate.\n            \n        #====================================================================================== \n        # Perform frequent evaluation within epoch.\n        # e.g. Validate for every 10 batches of training data (and also on the last batch).\n        #======================================================================================\n        if (i % 10 == 0) or ((i + 1) == len(train_data_loader)):\n            v_losses = validate_fn(val_data_loader, model, device)\n            val_loss = np.mean(v_losses)        # Average of all batches in val_data_loader.\n            \n            val_losses.append(val_loss)                \n            train_losses.append(train_loss/(i+1)) # Append the average train_loss.\n\n            rmse = val_loss\n            \n            # If there's improvement on the validation loss, save the model checkpoint.\n            # Else do early stopping if threshold is reached.\n            if rmse < best_rmse:            \n                torch.save(model.state_dict(), f\"roberta_base_fold_{fold}.pt\")\n                if swa_step: torch.save(swa_model.state_dict(), f\"roberta_base_swa_fold_{fold}.pt\")\n                print(f\"FOLD: {fold}, Epoch: {epoch}, Batch {i}, RMSE = {round(rmse,4)}, checkpoint saved.\")\n                best_rmse = rmse\n                early_stopping_counter = 0\n            else:\n                print(f\"FOLD: {fold}, Epoch: {epoch}, Batch {i}, RMSE = {round(rmse,4)}\")\n                early_stopping_counter += 1\n            if early_stopping_counter > EARLY_STOP_THRESHOLD:                \n                print(f\"Early stopping triggered!\")                \n                break\n    \n    return best_rmse, early_stopping_counter, train_losses, val_losses, lr_list, lr_list2","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.87151Z","iopub.execute_input":"2021-09-14T08:41:25.871861Z","iopub.status.idle":"2021-09-14T08:41:25.886071Z","shell.execute_reply.started":"2021-09-14T08:41:25.871827Z","shell.execute_reply":"2021-09-14T08:41:25.885285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lr_schedule(learning_rates):\n    x = np.arange(len(learning_rates))\n    plt.plot(x, learning_rates)\n    plt.title(f'Linear schedule')\n    plt.ylabel(\"Learning Rate\")\n    plt.xlabel(\"Training Steps\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.887474Z","iopub.execute_input":"2021-09-14T08:41:25.887877Z","iopub.status.idle":"2021-09-14T08:41:25.896567Z","shell.execute_reply.started":"2021-09-14T08:41:25.887841Z","shell.execute_reply":"2021-09-14T08:41:25.895779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_train_val_losses(train_losses, val_losses, fold):\n    x = np.arange(len(train_losses))\n    plt.plot(x, train_losses, label=\"training loss\", marker='o')\n    plt.plot(x, val_losses, label=\"validation loss\", marker='o')\n    plt.legend(loc=\"best\")   # to show the labels.\n    plt.title(f'Fold {fold}: roberta-base')    \n    plt.ylabel(\"RMSE\")\n    plt.xlabel(f\"Epoch\")    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.897866Z","iopub.execute_input":"2021-09-14T08:41:25.898264Z","iopub.status.idle":"2021-09-14T08:41:25.906412Z","shell.execute_reply.started":"2021-09-14T08:41:25.898231Z","shell.execute_reply":"2021-09-14T08:41:25.905584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Training","metadata":{}},{"cell_type":"code","source":"def run_training(df, model_head=\"pooler\", grouped_LLRD=True, warmup_steps=0,\n                 reinit_n_layers=0, swa_lr=None, frequent_eval=False):          \n    \"\"\"\n    model_head     : Accepted option is \"pooler\", \"attnhead\", or \"concatlayer\".\n    grouped_LLRD   : True: apply LLRD on grouped basis; False: apply LLRD on invidual layers.\n    warmup_steps   : Number of warmup steps (e.g. 50) to apply on the scheduler.\n    reinit_n_layers: Number of last layers (e.g. 5) to reinitialize on the model.\n    swa_lr         : If value is provided (e.g. 2e-6), apply Stochastic Weight Averaging.\n    frequent_eval  : If True, perform frequent evaluation within epochs.\n                     Remember to increase the EARLY_STOP_THRESHOLD (e.g. 50) if using frequent_eval.\n    \"\"\"    \n    EPOCHS = 5\n    FOLDS = [0]#, 1, 2, 3, 4]   # Set the list of folds you want to train\n    EARLY_STOP_THRESHOLD = 3    \n    TRAIN_BS = 16             # Training batch size     \n    VAL_BS = 64               # Validation batch size  \n    cv = []                   # A list to hold the cross validation scores\n        \n    swa_model = None\n    swa_scheduler = None\n    swa_start = 3\n            \n    #=========================================================================\n    # Prepare data and model for training\n    #=========================================================================\n    \n    for fold in FOLDS:\n        \n        set_random_seed(3377)\n        \n        # Initialize the tokenizer\n        tokenizer =  RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n        # Fetch training data\n        df_train = df[df[\"skfold\"] != fold].reset_index(drop=True)\n\n        # Fetch validation data\n        df_val = df[df[\"skfold\"] == fold].reset_index(drop=True)\n\n        # Initialize training dataset\n        train_dataset = MyDataset(texts = df_train[\"excerpt\"].values,\n                                  targets = df_train[\"target\"].values,\n                                  tokenizer = tokenizer)\n\n        # Initialize validation dataset\n        val_dataset = MyDataset(texts = df_val[\"excerpt\"].values,\n                                targets = df_val[\"target\"].values,\n                                tokenizer = tokenizer)\n\n        # Create training dataloader\n        train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n                                       shuffle = True, num_workers = 2)\n\n        # Create validation dataloader\n        val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n                                     shuffle = False, num_workers = 2)\n\n        # Initialize the cuda device (or use CPU if you don't have GPU)\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        # Load model and send it to the device.        \n        if model_head == \"pooler\":\n            model = MyModel(reinit_n_layers).to(device)\n        elif model_head == \"attnhead\":\n            model = MyModel_AttnHead().to(device)\n        elif model_head == \"concatlayer\":\n            model = MyModel_ConcatLast4Layers().to(device)\n        else:\n            raise ValueError(f\"Unknown model_head: {model_head}\") \n            \n        if swa_lr is not None: swa_model = AveragedModel(model).to(device)\n\n        # Get the AdamW optimizer\n        if grouped_LLRD is None:\n            optimizer = transformers.AdamW(model.parameters(), lr=1e-6)\n        elif grouped_LLRD==True:\n            optimizer, _ = roberta_base_AdamW_grouped_LLRD(model)\n        else:\n            optimizer, _ = roberta_base_AdamW_LLRD(model)\n        \n        # Calculate the number of training steps (this is used by scheduler).\n        # training steps = [number of batches] x [number of epochs].\n        train_steps = int(len(df_train) / TRAIN_BS * EPOCHS)    \n\n        # Get the learning rate scheduler    \n        scheduler = transformers.get_scheduler(\n                        \"linear\",    # Create a schedule with a learning rate that decreases linearly \n                                     # from the initial learning rate set in the optimizer to 0.\n                        optimizer = optimizer,\n                        num_warmup_steps = warmup_steps,\n                        num_training_steps = train_steps)    \n        \n        if swa_lr is not None:\n            swa_scheduler = SWALR(optimizer, swa_lr=swa_lr)\n            \n        #=========================================================================\n        # Training Loop - Start training the epochs\n        #=========================================================================\n\n        print(f\"===== FOLD: {fold} =====\")    \n        best_rmse = 999\n        early_stopping_counter = 0       \n        all_train_losses = []\n        all_val_losses = []\n        all_lr = []\n        all_lr2 = []\n\n        for epoch in range(EPOCHS):\n            \n            if frequent_eval and epoch >= 2:\n                \n                # Call the 'train_and_validate' function.\n                best_rmse, early_stopping_counter, train_losses, val_losses, lr_list, lr_list2 \\\n                    = train_and_validate(best_rmse, early_stopping_counter, fold, epoch, \n                                         train_data_loader, val_data_loader, \n                                         model, optimizer, device, scheduler,\n                                         EARLY_STOP_THRESHOLD, grouped_LLRD,\n                                         swa_model, swa_scheduler, \n                                         True if swa_lr is not None and (epoch>=swa_start) else False) \n                \n                all_train_losses.extend(train_losses)\n                all_val_losses.extend(val_losses)\n                \n                if early_stopping_counter > EARLY_STOP_THRESHOLD: break      \n                \n            else:\n\n                # Call the train function and get the training loss.\n                train_losses, lr_list, lr_list2 = train_fn(train_data_loader, model, optimizer, \n                                                           device, scheduler, grouped_LLRD,\n                                                           swa_model, swa_scheduler, \n                                                           True if swa_lr is not None and (epoch>=swa_start) else False)\n                train_loss = np.mean(train_losses)   \n                all_train_losses.append(train_loss)\n\n                # Perform validation and get the validation loss.\n                val_losses = validate_fn(val_data_loader, model, device)\n                val_loss = np.mean(val_losses)\n                all_val_losses.append(val_loss)    \n\n                rmse = val_loss\n\n                # If there's improvement on the validation loss, save the model checkpoint.\n                # Else do early stopping if threshold is reached.\n                if rmse < best_rmse:            \n                    torch.save(model.state_dict(), f\"roberta_base_fold_{fold}.pt\")\n                    if swa_lr is not None and (epoch>=swa_start):\n                        torch.save(swa_model.state_dict(), f\"roberta_base_swa_fold_{fold}.pt\")\n                    print(f\"FOLD: {fold}, Epoch: {epoch}, RMSE = {round(rmse,4)}, checkpoint saved.\")\n                    best_rmse = rmse\n                    early_stopping_counter = 0\n                else:\n                    print(f\"FOLD: {fold}, Epoch: {epoch}, RMSE = {round(rmse,4)}\")\n                    early_stopping_counter += 1\n                if early_stopping_counter > EARLY_STOP_THRESHOLD:                \n                    print(f\"Early stopping triggered!\")                \n                    break\n                    \n            all_lr.extend(lr_list)\n            all_lr2.extend(lr_list2)\n        \n        # Keep the best_rmse as cross validation score for the fold.\n        print(f\"Best RMSE: {round(best_rmse,4)}\")        \n        cv.append(best_rmse)\n        \n        if swa_lr is not None: torch.optim.swa_utils.update_bn(train_data_loader, swa_model)\n\n        # Plot the losses.\n        plot_train_val_losses(all_train_losses, all_val_losses, fold)\n        # Plot the lr schedule.\n        if grouped_LLRD is None: plot_lr_schedule(all_lr)   \n        \n        # Create dataframe to save learning rates for each layer.\n        cols = [\"embeddings\", \"layer0\", \"layer1\", \"layer2\", \"layer3\", \n                \"layer4\", \"layer5\", \"layer6\", \"layer7\", \n                \"layer8\", \"layer9\", \"layer10\", \"layer11\", \n                \"pooler\", \"regressor\"]        \n        df_lr = pd.DataFrame(all_lr2, columns=cols)\n        df_lr[\"fold\"] = int(fold)\n        df_lr.to_csv(f\"LR_fold{fold}.csv\", index=False)\n                \n    # Print the cross validation scores and their average.\n    cv_rounded = [ round(elem, 4) for elem in cv ] \n    print(f\"CV: {cv_rounded}\") \n    print(f\"Average CV: {round(np.mean(cv), 4)}\\n\") ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.908135Z","iopub.execute_input":"2021-09-14T08:41:25.908504Z","iopub.status.idle":"2021-09-14T08:41:25.934646Z","shell.execute_reply.started":"2021-09-14T08:41:25.90847Z","shell.execute_reply":"2021-09-14T08:41:25.933794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n        \ngrouped_LLRD = False   # True: apply LLRD on grouped basis; False: apply LLRD on invidual layers.\nwarmup_steps = 50      # Number of warmup steps (e.g. 50) to apply on the scheduler.\nreinit_n_layers = 5    # Number of last layers (e.g. 5) to reinitialize on the model.\nswa_lr = 2e-6          # Provide a value (e.g. 2e-6) to apply Stochastic Weight Averaging. Else input None.\nfrequent_eval = False  # If True, perform frequent evaluation within epochs.\n                       # Remember to increase the EARLY_STOP_THRESHOLD (e.g. 50) if using frequent_eval.\n\nif __name__ == \"__main__\":\n    run_training(df, model_head = \"pooler\", \n                 grouped_LLRD = grouped_LLRD, \n                 warmup_steps = warmup_steps,\n                 reinit_n_layers = reinit_n_layers,\n                 swa_lr = swa_lr,\n                 frequent_eval = frequent_eval\n                )    ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:41:25.938274Z","iopub.execute_input":"2021-09-14T08:41:25.938586Z","iopub.status.idle":"2021-09-14T08:47:09.149701Z","shell.execute_reply.started":"2021-09-14T08:41:25.938561Z","shell.execute_reply":"2021-09-14T08:47:09.148588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Layer-wise Learning Rate Decay","metadata":{}},{"cell_type":"code","source":"import plotly\nimport plotly.graph_objs as go\nimport plotly.express as px\n\ndef get_lr_trace(learning_rates, name=\"\", color=\"blue\"):    \n    return go.Scatter(            \n            x = list(range(0, len(learning_rates), 1)), \n            y = learning_rates, \n            texttemplate = \"%{y:.10f}\",\n            mode = 'lines',      # Can use 'lines' or 'markers+lines'\n            name = name,\n            marker = dict(color=color)\n        )\n\ndef plot_LLRD(df_LR, grouped_LLRD, swa_lr, fold=0):\n        \n    if grouped_LLRD == True:\n        colors = [\"lightskyblue\", \"gold\", \"green\", \"red\"]                 \n        cols = ['regressor', 'layer8', 'layer4', 'layer0']\n        cols_name = ['pooler & regressor', 'layer 8,9,10,11', 'layer 4,5,6,7', 'embeddings & layer 0,1,2,3']\n        title = \"Grouped Layer-wise Learning Rate Decay\"\n    else:\n        colors = [\"aqua\", \"lightskyblue\", \"teal\", \"blue\", \"navy\", \"gold\", \"orange\", \"olive\", \n                  \"lime\", \"green\", \"hotpink\", \"fuchsia\", \"blueviolet\", \"purple\", \"red\"]\n        cols = [col for col in df_LR.columns if col != 'fold']\n        cols_name = cols\n        cols.reverse()    # To show last layer at the top.\n        title = \"Layer-wise Learning Rate Decay\"\n\n    if swa_lr is not None: title = title + \" (with SWA)\"\n        \n    traces = []    \n    for i, col in enumerate(cols):\n        traces.append(get_lr_trace(df_LR[col], name=cols_name[i], color=colors[i]))\n\n    fig = go.Figure(\n        data = traces,\n        layout = go.Layout(\n            template = 'plotly_white',\n            title = {'text': title, 'font_family': 'Arial', 'font_size': 20, 'font_color': 'darkred'},\n            legend = {'itemclick': 'toggleothers'},\n            hovermode = 'x unified',  # can use 'x' or 'x unified'        \n            xaxis = {'exponentformat': 'none', 'showexponent': 'none', \n                     'tickfont': {'family': 'Arial', 'size': 14},\n                     'title': {'text': 'Steps', 'font_family': 'Arial', 'font_size': 16}\n                    },\n            yaxis = {'exponentformat': 'e', 'showexponent': 'all',\n                     'tickfont': {'family': 'Arial', 'size': 14},\n                     'title': {'text': 'Learning rate', 'font_family': 'Arial', 'font_size': 16}\n                    }\n        )\n    )\n\n    fig.show()\n    plotly.offline.plot(fig, filename=f'LR_fold{fold}.html', auto_open=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:47:09.151753Z","iopub.execute_input":"2021-09-14T08:47:09.152135Z","iopub.status.idle":"2021-09-14T08:47:09.16501Z","shell.execute_reply.started":"2021-09-14T08:47:09.152093Z","shell.execute_reply":"2021-09-14T08:47:09.164009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.float_format = '{:,.10f}'.format\n#pd.reset_option('display.float_format')\n\ndf_LR = pd.read_csv(\"LR_fold0.csv\")\ndf_LR","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:47:09.166462Z","iopub.execute_input":"2021-09-14T08:47:09.166872Z","iopub.status.idle":"2021-09-14T08:47:09.198985Z","shell.execute_reply.started":"2021-09-14T08:47:09.166837Z","shell.execute_reply":"2021-09-14T08:47:09.198107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_LLRD(df_LR, grouped_LLRD, swa_lr, fold=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:47:09.200186Z","iopub.execute_input":"2021-09-14T08:47:09.200521Z","iopub.status.idle":"2021-09-14T08:47:09.472134Z","shell.execute_reply.started":"2021-09-14T08:47:09.200488Z","shell.execute_reply":"2021-09-14T08:47:09.471362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_LR = pd.read_csv(\"LR_fold4.csv\")\nplot_LLRD(df_LR, grouped_LLRD, swa_lr, fold=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:47:09.473396Z","iopub.execute_input":"2021-09-14T08:47:09.473743Z","iopub.status.idle":"2021-09-14T08:47:09.52212Z","shell.execute_reply.started":"2021-09-14T08:47:09.473698Z","shell.execute_reply":"2021-09-14T08:47:09.5202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### If you're not able to see the plot, click the links below for:\n- [sample of Layer-wise Learning Rate Decay](LLRD_v41.html)\n- [sample of Layer-wise Learning Rate Decay (with warm-up steps)](LLRD_warmup50_v40.html)\n- [sample of Layer-wise Learning Rate Decay (with warm-up steps and SWA)](LLRD_warmup50_swa_v30.html)\n\n- [sample of Grouped Layer-wise Learning Rate Decay](GroupedLLRD_v42.html) \n- [sample of Grouped Layer-wise Learning Rate Decay (with warm-up steps and SWA)](GroupedLLRD_warmup50_swa_v32.html)\n","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')    ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:47:09.523363Z","iopub.status.idle":"2021-09-14T08:47:09.524166Z"},"trusted":true},"execution_count":null,"outputs":[]}]}